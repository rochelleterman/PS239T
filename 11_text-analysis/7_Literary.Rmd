---
title: "Literary Concerns and Word Order Dependent Methods"
author: "PS239T"
date: "Fall 2015"
output: html_document
---

**CREDITS**: These materials were written by Teddy Roland.

### Setup Environment

```{r message=F}
rm(list=ls())
setwd("~/Dropbox/berkeley/Git-Repos/PS239T/11_text-analysis")

library(tm)
library(RTextTools) # a machine learning package for text classification written in R
library(qdap) # quantiative discourse analysis
library(entropy) # tools applying Information Theory

```

# 1. Alliteration: Counting

Unlike many computational studies on novels, when we look at poetry -- and especially poetic form -- we are often interested not in the words themselves but their sounds. Essentially, this is the difference between orthography and phonology. Although there are many languages in which there is a one-to-one mapping between letters and sounds, this is not the case for English. Fortunately, there are several publicly available pronouncing dictionaries, such as the CMU Pronouncing Dictionary and Nettalk. These dictionaries include information about syllable units, stress, and phonemes -- and in some cases multiple pronunciations.

Of course, when we study poetic form we are interested not only in sounds but their sequences and repetitions, which means that a bag-of-words model might only have limited application. In practice, the methods used to study poetic form typically begin with dictionary lookup methods (not unlike we did with sentiment analysis) and incorporate some kind of pattern recognition, increasingly using statistics. This lesson will walk through some of those lookups and start to think about strategies for finding patterns.

```{r}
# Load Data
sonnets.df <- read.csv('Data/shakes-sonnets.tsv', header=FALSE, sep="\t", as.is=TRUE)

# Take a peek: Each row contains one sonnet and each column contains one line
head(truncdf(sonnets.df),10)
```

"unlist" is a function to take entries from a two-dimensional structures like a data frame and turn them into a one-dimensional list. This will come in handy when we want to perform an operation on every entry.

```{r}
# Get a list of all lines in the sonnets
all_lines = unlist(sonnets.df)
```

"syllable_sum" in the qdap package is a powerful function for syllable counting. It first checks the Nettalk pronouncing dictionary to check whether it knows the number of syllables already, and failing that, uses machine learning to guess the number of syllables based on orthography.

```{r}
# Count the number of syllables per line in all sonnets
all_sylls = syllable_sum(all_lines)

# Check results
all_sylls
```

Shakespeare's sonnets are a famous and extensive example of iambic pentameter, which is in part characterized by a ten syllable line. We can check the consistency of that syllable count, using a couple basic statistics.

Note that when we checked all_sylls, r had introduced an "NA" wherever it had expected to find a line of poetry but didn't (since the number of lines varies across in the cycle). We will remove these before our statistical measures.

```{r}
# Median syllable count (Half of all counts above, half below)
median(all_sylls, na.rm=TRUE)

# Max/Min syllable counts
range(all_sylls, na.rm=TRUE)

# Inter-Quartile Range (Difference across the middle fifty-percent of all counts)
IQR(all_sylls, na.rm=TRUE)
```
Let's look more closely at just one sonnet to see what's happening under the hood.

```{r}
# Get a vector of lines in the eighth sonnet
eighth_sonn <- unlist(sonnets.df[8,])
eighth_sonn

# Take a look at the syllable counts
eighth_syll <- syllable_sum(eighth_sonn)
eighth_syll

# Get those stats
median(eighth_syll, na.rm = T)
IQR(eighth_syll, na.rm = T)

get.iqr <- function(poem){
  sonn <- unlist(sonnets.df[poem,])
  syll <- syllable_sum(sonn)
  return(IQR(syll, na.rm = T))
}

seq <- 1:154
x <- sapply(seq, get.iqr)
which(x >= 1)
```

EXERCISE: Find another sonnet that has an IQR greater than 0.


"syllable_sum" relies on a broader function called "syllable_count" that gives some more information.

```{r}

# Check the first line
eighth_sonn[[1]]

# Get syllable information for just the first line.
syllable_count(eighth_sonn[[1]])
```

Okay, now let's break the function. "syllable_sum" is very good at counting syllables, but let's say we wanted to study alliteration in the sonnets, which requires a little more information about pronunciations.

The documentation indicates that the function begins with a dictionary lookup from the "Nettalk Corpus", that is publicly available on the internet.

```{r}
# Documentation for syllable_sum
help(syllable_sum)

# Load the Nettalk dictionary
nettalk.df <- read.csv('Data/nettalk.data.tsv', header=FALSE, sep="\t", as.is=TRUE)

# Take a look
head(nettalk.df)
```

If we're going to perform our own lookups in the dictionary, we have to do some legwork that "syllable_sum" had done under the hood.

```{r}
# Tokenize the first line of the eighth sonnet
tokens <- gsub("[[:punct:]]", " ", eighth_sonn[[1]])
tokens <- MC_tokenizer(tokens)
tokens <- tolower(tokens)

# Create a function to look up the entry for a given word word
dict.lookup <- function(x) nettalk.df[nettalk.df[1]==as.vector(x)][2]

# Do the look-ups over each token
pronunciations <- sapply(tokens, dict.lookup)

# Check the results
pronunciations
```

Now that we have information about the pronunciation about *most* syllables, we can start to think about alliteration.

```{r}
# Create a function to get the first phoneme of each word
initial <- function(x) substr(x,1,1)

# Get the first phoneme from each word's pronunciation
first.phoneme <- sapply(pronunciations, initial)

# Count the frequency of each phoneme
table(first.phoneme)
```

EXERCISE: Using the table returned above, make R count the following:

1. How many syllables contain alliteration?
2. How many different alliterating sounds are there?

(Hint: The table is a numeric vector.)

CHALLENGE: Rewrite the script so that it returns a vector containing the number of alliterating syllables in every line of the eighth sonnet.

# 2. Part-of-Speech Tagging

Part of Speech tagging is used for a great variety of tasks, including Named Entity Recognition (NER) which can help to identify characters in a text as well as higher order problems like determining who is the subject or object of an action. Another type of question we can ask is about the grammatical sophistication of a text. One version of this is treated as the text's "reading level," while in literature this may be thought of as the grammatical mode by which a text explores its content.

```{r}
# Load corpus of novels
documents<-Corpus(DirSource("Data/POS-fiction"))

# Take a peek at the novels
# The double brackets call up different novels: 1 = A Game of Thrones; 2 = Beloved
head(as.character(documents$content[[1]]))
```

Importing the novel had introduced section breaks across paragraphs that we don't need, but on the other hand, we want to split the text into sentences. 

```{r}
# Combine sections of text
one.string <- paste(documents$content[[1]], collapse = " ")

# Split into sentences
sentences <- sent_detect(one.string)
```

Now that the text is divided into more grammatically meaningful units, we can start looking at Parts of Speech.

```{r}
# Check out the POS Tags
pos_tags()

# Run Part-of-Speech tagger over the novels
# posd <- pos(sentences)

# if pressed for time
posd <- pos(head(sentences))

# check it out
posd
```

The "pos" function returns a great deal of information, including not only the text itself, but full sentences in which each word appears with its tag, and counts of the tags themselves. We're especially interested in the kind of information reported in the value "POStagged".

```{r}
# visualize parts of speech per sentence
plot(posd)

# view sentences with tags in-line, list of tags by sentence, & word counts
posd$POStagged
```

EXERCISE: Use R to retrive the vector of tagged sentences from 'posd'

CHALLENGE: Count the number of instances of each POS tag.

The in-line tags might be useful if we wished to, say, idenitify the frequencies of nouns across our texts. For this particular project, we'll use the list of tags to compute bigram frequencies.

```{r}
# Get list of tags for each sentence
posd.list <- posd[[2]][[2]]

# Shorten tags to two characters
posd.list<-lapply(posd.list, function(x) sapply(x, function(x) substr(x,1,2)))

# Get bigrams of POS tags
pos.grams <- lapply(posd.list, function(x) ngrams(paste(x, collapse = " "), 2))

# Get the list of bigrams for each sentence
gram.list <-sapply(pos.grams, function(x) x$all_n$n_2)

# Pull out all the bigrams into a single list, since we no longer are interested in sentence divisions
gram.list <- unlist(gram.list, recursive=FALSE)

# Join bigrams with a hyphen into a single unit
gram.list<-sapply(gram.list, function(x) paste(x, sep=' ',collapse = '-'))

# Count the instances of all bigram units
gram.counts <- as.vector(table(gram.list))

# Take a look
gram.counts
```

And now that we have counted the total instances of each bigram, we can calculate the degree of grammatical entropy -- something like its complexity -- using Shannon's formula for informational entropy: -SUM(p(x)*log(p(x)))

```{r}

# Get the probability of randomly choosing a particular bigram from the text
prob<-gram.counts/sum(gram.counts)

# Caculate the Shannon entropy of the text
Shannon.entropy <- -sum(log2(prob)*prob)
Shannon.entropy

# ...or we can just use the entropy function
entropy(gram.counts, unit='log2')
```

EXERCISE: Find the grammatical entropy of a pair of texts from your own corpus. How do they compare to one another?
